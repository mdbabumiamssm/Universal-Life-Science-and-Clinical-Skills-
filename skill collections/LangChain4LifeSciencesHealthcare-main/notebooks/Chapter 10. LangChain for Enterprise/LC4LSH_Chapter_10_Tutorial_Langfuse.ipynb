{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Code to Chapter 10 of LangChain for Life Science and Healthcare book, by Dr. Ivan Reznikov\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ysMJ3XFQto_Asx4y-9E0etJEcKF-hsrD?usp=sharing)\n",
    "\n",
    "\n",
    "## Langfuse Tutorial - Building an Observable RAG System\n",
    "\n",
    "This tutorial demonstrates how to build a Retrieval-Augmented Generation (RAG) system with **Langfuse** for comprehensive observability and monitoring. Langfuse is an open-source LLM engineering platform that provides tracing, evaluation, and analytics for AI applications.\n",
    "\n",
    "## What You'll Learn\n",
    "- Setting up Langfuse for LLM observability\n",
    "- Building a RAG system using LangChain and LangGraph\n",
    "- Implementing tracing and monitoring for production AI systems\n",
    "- Processing PDF documents for knowledge retrieval"
   ],
   "metadata": {
    "id": "T9UAd0urWWWp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, we'll install all the necessary packages for our RAG system with Langfuse integration."
   ],
   "metadata": {
    "id": "7OahxqisW3JS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQbU8GUfO-qZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "90ccdffc-2cb3-42bf-f93b-28f315560332"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q langfuse langchain langgraph langchain_community langchain_openai openai pypdf"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip freeze | grep \"lang\\|openai\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_1qaj5uadpK",
    "outputId": "26418e48-64cc-49d4-b142-e121093e7380"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "google-ai-generativelanguage==0.6.15\n",
      "google-cloud-language==2.17.2\n",
      "langchain==0.3.26\n",
      "langchain-community==0.3.27\n",
      "langchain-core==0.3.71\n",
      "langchain-openai==0.3.28\n",
      "langchain-text-splitters==0.3.8\n",
      "langcodes==3.5.0\n",
      "langfuse==3.2.1\n",
      "langgraph==0.5.4\n",
      "langgraph-checkpoint==2.1.1\n",
      "langgraph-prebuilt==0.5.2\n",
      "langgraph-sdk==0.1.74\n",
      "langsmith==0.4.8\n",
      "language_data==1.3.0\n",
      "libclang==18.1.1\n",
      "openai==1.97.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Configuration and Authentication\n",
    "\n",
    "Setting up environment variables for API access. We're using Google Colab's userdata for secure credential management.\n",
    "\n",
    "**Important Notes:**\n",
    "- Replace the userdata calls with your actual API keys if not using Colab\n",
    "- Choose the appropriate Langfuse host based on your region\n",
    "- Keep your API keys secure and never commit them to version control"
   ],
   "metadata": {
    "id": "djgbIbLgW8AB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import openai\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"LC4LS_OPENAI_API_KEY\")\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = userdata.get(\"LANGFUSE_PUBLIC_KEY\")\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = userdata.get(\"LANGFUSE_SECRET_KEY\")\n",
    "\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # US region"
   ],
   "metadata": {
    "id": "cbeoRbhU1LdE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Initialize Langfuse Callback Handler\n",
    "\n",
    "The CallbackHandler is the core component that enables tracing of your LLM applications.\n",
    "\n",
    "**What this does:**\n",
    "- Creates a callback handler that will automatically trace all LLM interactions\n",
    "- Verifies that your API keys and connection to Langfuse are working\n",
    "- If successful, you should see a confirmation message"
   ],
   "metadata": {
    "id": "m7-EQDlbXBsQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "divRadPqZouF"
   },
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FVbg1RWoT8W"
   },
   "outputs": [],
   "source": [
    "# Tests the SDK connection with the server\n",
    "# langfuse_handler.auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Data Preparation - Downloading Research Paper\n",
    "\n",
    "We'll download a research paper about protein generative models to use as our knowledge base.\n",
    "\n",
    "**Key Points:**\n",
    "- We create a data directory to organize our files\n",
    "- Headers are added to avoid potential blocking by the server\n",
    "- The paper focuses on watermarking protein generative models"
   ],
   "metadata": {
    "id": "zntRUzVdXGlW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "os.makedirs(\"./data\", exist_ok=True)"
   ],
   "metadata": {
    "id": "-t0mlJwt9Hgs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://github.com/IvanReznikov/LangChain4LifeScience/blob/main/data/articles/2410.20354v4.pdf\",\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://raw.githubusercontent.com/IvanReznikov/LangChain4LifeScience/refs/heads/main/data/articles/2410.20354v4.pdf\",\n",
    "    headers=headers,\n",
    ")\n",
    "\n",
    "pdf_path = \"./data/article.pdf\"\n",
    "with open(pdf_path, \"wb\") as f:\n",
    "    f.write(response.content)"
   ],
   "metadata": {
    "id": "SoYFukAz0wbQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Import Required Libraries\n",
    "\n",
    "Now we'll import all the necessary components for building our RAG system with LangGraph.\n",
    "\n",
    "**Library Overview:**\n",
    "- **LangChain Hub**: Access to pre-built prompts and chains\n",
    "- **Document Loaders**: For processing various file formats\n",
    "- **Text Splitters**: Breaking documents into manageable chunks\n",
    "- **LangGraph**: For building stateful, multi-step AI workflows\n",
    "- **Vector Stores**: For semantic search and retrieval\n",
    "- **OpenAI Components**: LLM and embedding models"
   ],
   "metadata": {
    "id": "JVVg4sqYXLf-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INdC3WvLO-qb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9ddf8072-ee20-42d8-c048-00eb87dde844"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Initialize the Language Model\n",
    "\n",
    "We'll use GPT-4o-mini for our RAG system - it's cost-effective and performs well for most use cases.\n",
    "\n",
    "**Why GPT-4o-mini?**\n",
    "- Balanced performance and cost\n",
    "- Good reasoning capabilities for Q&A tasks\n",
    "- Fast response times\n",
    "- Suitable for production RAG applications"
   ],
   "metadata": {
    "id": "60VkABmGXQSD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "metadata": {
    "id": "3D0vepMLjlKF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Define Application State\n",
    "\n",
    "LangGraph uses a state-based approach. We define what information flows between steps.\n",
    "\n",
    "**State Management:**\n",
    "- **question**: The user's query that needs to be answered\n",
    "- **context**: Documents retrieved from the vector store that are relevant to the question\n",
    "- **answer**: The final generated response combining the question and context"
   ],
   "metadata": {
    "id": "qh4sA9m2XflU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ],
   "metadata": {
    "id": "_lRTnhh5jYK4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Define Application Steps\n",
    "\n",
    "Our RAG system has two main steps: retrieve relevant documents and generate an answer.\n",
    "\n",
    "**How it works:**\n",
    "1. **Retrieve**: Searches the vector store for documents similar to the question\n",
    "2. **Generate**: Combines retrieved context with the question and generates a comprehensive answer\n",
    "\n",
    "### Step 1: Document Retrieval"
   ],
   "metadata": {
    "id": "ThhxsBGzXktq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}"
   ],
   "metadata": {
    "id": "5GoTMku4Xm0_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Answer Generation"
   ],
   "metadata": {
    "id": "myF_ZsIwXpO4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ],
   "metadata": {
    "id": "7U8yW-hYXpiU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Document Processing Pipeline\n",
    "\n",
    "Now we'll load, split, and index our research paper for retrieval.\n",
    "\n",
    "**Why these parameters?**\n",
    "- **chunk_size=1000**: Large enough to contain meaningful information, small enough for efficient processing\n",
    "- **chunk_overlap=100**: Ensures important information isn't lost at chunk boundaries"
   ],
   "metadata": {
    "id": "RDMDBEpYXwbj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "loader = PyPDFLoader(\"./data/article.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)"
   ],
   "metadata": {
    "id": "Bjdrnmky03AJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vector Store Creation\n",
    "\n",
    "**Embedding Model Choice:**\n",
    "- **text-embedding-3-large**: OpenAI's most capable embedding model\n",
    "- Provides high-quality semantic representations\n",
    "- Better retrieval accuracy compared to smaller models"
   ],
   "metadata": {
    "id": "BEAGZ77MX2JS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "vector_store = InMemoryVectorStore(OpenAIEmbeddings(model=\"text-embedding-3-large\"))\n",
    "vector_store.add_documents(documents=docs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71ZHKWDmYGGz",
    "outputId": "a802cbf1-2b44-42d6-f92f-3e3e7a848f5d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['076c1114-ef20-4dad-9bbb-9de8e176bc62',\n",
       " 'd4dd5192-bb8f-4763-b6fc-614d92909dd8',\n",
       " '10104c72-9c2e-4408-8e10-38b666c5620d',\n",
       " 'd051d16e-9bcb-4653-80f7-88dae27685f4',\n",
       " '125660ee-fd98-4d7e-aecc-0b212d07acd6',\n",
       " '6a6935b7-ce1f-463a-a7f9-a61f008dfcca',\n",
       " 'fbe9d2e4-4994-40cf-b192-d56e74251c4f',\n",
       " '60456c74-00f5-413d-90d8-a2bfd730f0fa',\n",
       " 'f7023ed1-7422-4306-91ee-bb905e054fea',\n",
       " '89af3f2d-e589-4555-b04b-36572ba7e06b',\n",
       " 'c2b426a7-d357-4158-80ba-401d009a817b',\n",
       " 'fa8c89db-bbc9-4c18-872d-c25a5f9d7df8',\n",
       " '91cf4fa5-64f9-4302-bbb8-c955fb583f24',\n",
       " '97dedee4-1884-4af0-b7b8-9399c43eb12e',\n",
       " '0b6f252d-ac99-457e-8650-7616408495e0',\n",
       " 'bebdad74-5392-49a4-bedb-08c850917b95',\n",
       " '5c3d0c75-4954-41b7-8feb-64a8a8f24faa',\n",
       " '3a193f99-6467-45e6-8baf-0bd060d0e409',\n",
       " '025934d2-c878-43a4-9f17-e370bf49bb17',\n",
       " '3fe3d524-1022-4ffe-aa4f-06eba627186a',\n",
       " '3321fe72-e918-4a77-9ce7-f3311e629c6a',\n",
       " 'ded81913-9c85-4f1e-bb17-2bec3b319109',\n",
       " 'a6843812-ffc5-45ce-a6e7-4023b05f6354',\n",
       " '148301e5-4586-4e11-b60f-7bc8581a8a7e',\n",
       " '6ff45b71-cf01-4fda-b045-4450383153db',\n",
       " '945e0364-fd87-4479-95d3-4090f234c167',\n",
       " 'a7a6322e-a6fb-4636-8c55-73a3de9d72ad',\n",
       " '8bb8fea8-22b3-4d14-9d9d-3e0b4f23acfa',\n",
       " '72391d99-fc8c-4bdf-aaf7-95a0ed8d095b',\n",
       " '70c3f4ca-42d5-411b-abba-9a6d2638d478',\n",
       " 'ba4f0fc4-35dd-48e6-9101-c9feb2d7b8c8',\n",
       " '3db2dc90-2c82-4c4d-a89b-66e8170ee92a',\n",
       " 'bbafef14-15ff-4eb7-9a2d-624596b14d8d',\n",
       " 'a3c9a72d-961e-483a-abe2-188833e23648',\n",
       " 'befdb448-675a-46cd-82a4-fd4c137466f4',\n",
       " 'cc0328ff-e273-4292-851d-d6ce2846979d',\n",
       " '7207ecfc-8e54-458b-b635-233938aee4d6',\n",
       " '5ddb9849-1829-4433-9b9a-e70f1f46e632',\n",
       " '73b97378-e767-4648-b789-285cc33d45a5',\n",
       " '269ec121-a227-4737-9b08-f15fea4bbd46']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RAG Prompt Template\n",
    "\n",
    "This prompt template is specifically designed for RAG applications and includes instructions for using retrieved context effectively."
   ],
   "metadata": {
    "id": "Pl8UPqY1YDDF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ],
   "metadata": {
    "id": "3U3fO7hM03F_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "47d14772-acda-4943-c101-3de687201b60"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Build the LangGraph Application\n",
    "\n",
    "Now we'll create our RAG pipeline using LangGraph's state-based approach.\n",
    "\n",
    "**Graph Structure:**\n",
    "1. **START** → **retrieve**: Begin by finding relevant documents\n",
    "2. **retrieve** → **generate**: Use retrieved documents to generate answer\n",
    "3. The state flows through each step, accumulating information"
   ],
   "metadata": {
    "id": "IiSApvdcYOIF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ],
   "metadata": {
    "id": "CCNuQPfyYOcA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11. Test the RAG System with Monitoring\n",
    "\n",
    "Let's test our system with a question about the research paper content.\n",
    "\n",
    "**What happens here:**\n",
    "1. The question is processed through our RAG pipeline\n",
    "2. Langfuse automatically traces each step (retrieval, generation)\n",
    "3. All LLM calls, embeddings, and intermediate results are logged\n",
    "4. You can view detailed traces in your Langfuse dashboard\n"
   ],
   "metadata": {
    "id": "TEGxI2R2YUbQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"What are the benefits of watermarking protein generative models?\"\n",
    "\n",
    "response = graph.invoke({\"question\": query}, config={\"callbacks\": [langfuse_handler]})"
   ],
   "metadata": {
    "id": "NHIHs-vS03Io"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(response[\"answer\"])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYjW0QBTafT9",
    "outputId": "2e8c5298-e7a8-4718-f283-5f45fb3cd8c1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The benefits of watermarking protein generative models include enabling copyright authentication and tracking of generated protein structures. The proposed FoldMark method effectively embeds user-specific information while maintaining the original quality of the protein structures. Additionally, it offers robustness against post-processing and adaptive attacks, addressing ethical concerns in generative AI applications for protein design.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "0G3YuBDMaf39"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
