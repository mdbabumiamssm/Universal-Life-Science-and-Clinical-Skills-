{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kww5B_vXO-qZ"
   },
   "source": [
    "## Code to Chapter 10 of LangChain for Life Science and Healthcare book, by Dr. Ivan Reznikov\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1j2VckKCbhNzp50nhjsUHyM4L1Oea-Gdn?usp=sharing)\n",
    "\n",
    "## Haystack Tutorial - Building a RAG System\n",
    "\n",
    "This tutorial demonstrates how to build a complete Retrieval-Augmented Generation (RAG) system using Haystack 2.0. We'll create a pipeline that can process PDF documents, split them into chunks, embed them, store them in a vector database, and then answer questions based on the content.\n",
    "\n",
    "## What is Haystack?\n",
    "\n",
    "Haystack is an open-source framework for building production-ready LLM applications. It provides modular components that can be combined into pipelines for tasks like document processing, retrieval, and generation. In this tutorial, we'll build a RAG system that can answer questions about scientific papers."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing Required Dependencies\n",
    "\n",
    "First, we need to install Haystack 2.0 and its dependencies. The key packages we're installing are:\n",
    "- `haystack-ai`: The main Haystack framework\n",
    "- `datasets`: For data handling utilities\n",
    "- `sentence-transformers`: For text embeddings\n",
    "- `openai`: For OpenAI API integration\n",
    "- `pypdf`: For PDF document processing"
   ],
   "metadata": {
    "id": "valK-z0KdDmc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQbU8GUfO-qZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "654af8ef-f672-44f3-a2aa-d83f78257b39"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/530.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m530.1/530.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/313.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#!pip install haystack-ai datasets>=2.6.1 sentence-transformers>=3.0.0 openai pypdf\n",
    "!pip install -q haystack-ai datasets sentence-transformers openai pypdf"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip freeze | grep \"haystack\\|openai\\|sentence-transformers\\|datasets\""
   ],
   "metadata": {
    "id": "ToRpJ-ltai32",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e9a83278-3ac9-466b-ba9e-b2d4574941db"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "datasets==4.0.0\n",
      "haystack-ai==2.15.2\n",
      "haystack-experimental==0.12.0\n",
      "openai==1.97.1\n",
      "sentence-transformers==4.1.0\n",
      "tensorflow-datasets==4.9.9\n",
      "vega-datasets==0.9.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting Up OpenAI API Configuration\n",
    "\n",
    "We need to configure the OpenAI API key for embedding generation and LLM queries. In Google Colab, we can securely store API keys using the userdata feature."
   ],
   "metadata": {
    "id": "oPx27OkfdGiH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import openai\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"LC4LS_OPENAI_API_KEY\")"
   ],
   "metadata": {
    "id": "cbeoRbhU1LdE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Important**: Make sure you've added your OpenAI API key to Colab's secrets (ðŸ”‘ icon in the left sidebar) before running this cell."
   ],
   "metadata": {
    "id": "PIusmJFAdLLq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Downloading Sample Document\n",
    "\n",
    "We'll download a scientific paper to use as our knowledge base. This example uses a paper about protein generative models from a GitHub repository."
   ],
   "metadata": {
    "id": "H064MsvhdOo6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "os.makedirs(\"./data\", exist_ok=True)"
   ],
   "metadata": {
    "id": "-t0mlJwt9Hgs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://github.com/IvanReznikov/LangChain4LifeScience/blob/main/data/articles/2410.20354v4.pdf\",\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://raw.githubusercontent.com/IvanReznikov/LangChain4LifeScience/refs/heads/main/data/articles/2410.20354v4.pdf\",\n",
    "    headers=headers,\n",
    ")\n",
    "\n",
    "pdf_path = \"./data/article.pdf\"\n",
    "with open(pdf_path, \"wb\") as f:\n",
    "    f.write(response.content)"
   ],
   "metadata": {
    "id": "SoYFukAz0wbQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Haystack Components\n",
    "\n",
    "Now we'll import all the necessary Haystack components for building our RAG pipeline. Each component serves a specific purpose in the document processing and retrieval workflow.\n",
    "\n",
    "**Component Overview**:\n",
    "- **DocumentWriter**: Stores processed documents in the document store\n",
    "- **PyPDFToDocument/TextFileToDocument**: Convert different file formats to Haystack documents\n",
    "- **DocumentSplitter**: Breaks documents into smaller chunks for better retrieval\n",
    "- **DocumentCleaner**: Removes unwanted characters and normalizes text\n",
    "- **FileTypeRouter**: Routes files to appropriate converters based on MIME type\n",
    "- **DocumentJoiner**: Combines documents from multiple sources\n",
    "- **OpenAIDocumentEmbedder**: Creates vector embeddings for documents\n",
    "- **InMemoryDocumentStore**: Stores documents and their embeddings in memory"
   ],
   "metadata": {
    "id": "_mR-uLDedb5v"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INdC3WvLO-qb"
   },
   "outputs": [],
   "source": [
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.converters import PyPDFToDocument, TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the Document Processing Pipeline\n",
    "\n",
    "This section creates a comprehensive pipeline that handles the entire document processing workflow, from file input to storing embedded chunks in the document store."
   ],
   "metadata": {
    "id": "7cKUv2QJdrf9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "document_store = InMemoryDocumentStore()\n",
    "file_type_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\"])\n",
    "text_file_converter = TextFileToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "document_joiner = DocumentJoiner()"
   ],
   "metadata": {
    "id": "T10UlurjdsPn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Key Configuration Choices**:\n",
    "- **Split length (150 words)**: Balances context preservation with retrieval precision\n",
    "- **Split overlap (50 words)**: Ensures important information isn't lost at chunk boundaries\n",
    "- **text-embedding-3-large**: OpenAI's most capable embedding model for better semantic understanding"
   ],
   "metadata": {
    "id": "icLAgt-ndwFd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "document_cleaner = DocumentCleaner()\n",
    "document_splitter = DocumentSplitter(\n",
    "    split_by=\"word\", split_length=150, split_overlap=50\n",
    ")"
   ],
   "metadata": {
    "id": "Bjdrnmky03AJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "document_embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-large\")\n",
    "document_writer = DocumentWriter(document_store)"
   ],
   "metadata": {
    "id": "GAm218lL03DR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assembling the Pipeline\n",
    "\n",
    "Now we'll create the pipeline and connect all components in the correct order:"
   ],
   "metadata": {
    "id": "tO26L59RdzCX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "preprocessing_pipeline = Pipeline()\n",
    "preprocessing_pipeline.add_component(instance=file_type_router, name=\"file_type_router\")\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=text_file_converter, name=\"text_file_converter\"\n",
    ")\n",
    "preprocessing_pipeline.add_component(instance=pdf_converter, name=\"pypdf_converter\")\n",
    "preprocessing_pipeline.add_component(instance=document_joiner, name=\"document_joiner\")\n",
    "preprocessing_pipeline.add_component(instance=document_cleaner, name=\"document_cleaner\")\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=document_splitter, name=\"document_splitter\"\n",
    ")\n",
    "preprocessing_pipeline.add_component(\n",
    "    instance=document_embedder, name=\"document_embedder\"\n",
    ")\n",
    "preprocessing_pipeline.add_component(instance=document_writer, name=\"document_writer\")"
   ],
   "metadata": {
    "id": "3U3fO7hM03F_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pipeline Flow Explanation**:\n",
    "1. **File Type Router** â†’ Routes files to appropriate converters\n",
    "2. **Converters** â†’ Extract text from PDFs or text files\n",
    "3. **Document Joiner** â†’ Combines all documents into a single stream\n",
    "4. **Document Cleaner** â†’ Normalizes and cleans the text\n",
    "5. **Document Splitter** â†’ Creates overlapping chunks\n",
    "6. **Document Embedder** â†’ Generates vector embeddings\n",
    "7. **Document Writer** â†’ Stores everything in the document store"
   ],
   "metadata": {
    "id": "HtXmVigOfO4x"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "preprocessing_pipeline.connect(\n",
    "    \"file_type_router.text/plain\", \"text_file_converter.sources\"\n",
    ")\n",
    "preprocessing_pipeline.connect(\n",
    "    \"file_type_router.application/pdf\", \"pypdf_converter.sources\"\n",
    ")\n",
    "preprocessing_pipeline.connect(\"text_file_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"pypdf_converter\", \"document_joiner\")\n",
    "preprocessing_pipeline.connect(\"document_joiner\", \"document_cleaner\")\n",
    "preprocessing_pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
    "preprocessing_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
    "preprocessing_pipeline.connect(\"document_embedder\", \"document_writer\")"
   ],
   "metadata": {
    "id": "NHIHs-vS03Io",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "934cc8bf-411b-410c-911f-93e9a3ab2375"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7cc17c2147d0>\n",
       "ðŸš… Components\n",
       "  - file_type_router: FileTypeRouter\n",
       "  - text_file_converter: TextFileToDocument\n",
       "  - pypdf_converter: PyPDFToDocument\n",
       "  - document_joiner: DocumentJoiner\n",
       "  - document_cleaner: DocumentCleaner\n",
       "  - document_splitter: DocumentSplitter\n",
       "  - document_embedder: OpenAIDocumentEmbedder\n",
       "  - document_writer: DocumentWriter\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - file_type_router.text/plain -> text_file_converter.sources (List[Union[str, Path, ByteStream]])\n",
       "  - file_type_router.application/pdf -> pypdf_converter.sources (List[Union[str, Path, ByteStream]])\n",
       "  - text_file_converter.documents -> document_joiner.documents (List[Document])\n",
       "  - pypdf_converter.documents -> document_joiner.documents (List[Document])\n",
       "  - document_joiner.documents -> document_cleaner.documents (List[Document])\n",
       "  - document_cleaner.documents -> document_splitter.documents (List[Document])\n",
       "  - document_splitter.documents -> document_embedder.documents (List[Document])\n",
       "  - document_embedder.documents -> document_writer.documents (List[Document])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing the Documents\n",
    "\n",
    "Let's run the preprocessing pipeline to process our downloaded PDF.\n",
    "\n",
    "**What Happens Here**:\n",
    "- The pipeline scans the `data` directory\n",
    "- Identifies `article.pdf` as a PDF file\n",
    "- Extracts text from the PDF\n",
    "- Cleans and splits the text into chunks\n",
    "- Generates embeddings for each chunk\n",
    "- Stores everything in the in-memory document store\n",
    "\n",
    "**Expected Output**: You should see processing logs and confirmation that documents have been embedded and stored."
   ],
   "metadata": {
    "id": "8rRxrWeXfRFX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "preprocessing_pipeline.run({\"file_type_router\": {\"sources\": [\"data\"]}})"
   ],
   "metadata": {
    "id": "6x2NiM_o1oLY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bad39fa0-0b8f-4e5f-b28b-1f5a74048ce5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'file_type_router': {'unclassified': [PosixPath('data')]}}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the Question-Answering Pipeline\n",
    "\n",
    "Now we'll create a second pipeline for answering questions using the processed documents. This implements the retrieval and generation components of our RAG system.\n",
    "\n",
    "**Component Choices**:\n",
    "- **gpt-4o-mini**: Balanced performance and cost for question answering\n",
    "- **ChatPromptBuilder**: Handles prompt templating with Jinja2 syntax\n",
    "- **Template Design**: Clearly separates context from question for better LLM performance\n"
   ],
   "metadata": {
    "id": "dXlDlKcMfYsh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "llm = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
    "prompt_builder = ChatPromptBuilder()\n",
    "template = [\n",
    "    ChatMessage.from_user(\n",
    "        \"\"\"\n",
    "Answer the questions based on the given context.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ question }}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    )\n",
    "]"
   ],
   "metadata": {
    "id": "OCeqUPIFfefr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assembling the QA Pipeline\n",
    "\n",
    "**QA Pipeline Flow**:\n",
    "1. **Text Embedder** â†’ Converts question to vector embedding\n",
    "2. **Retriever** â†’ Finds most relevant document chunks using similarity search\n",
    "3. **Prompt Builder** â†’ Creates formatted prompt with context and question\n",
    "4. **LLM** â†’ Generates answer based on retrieved context"
   ],
   "metadata": {
    "id": "qOazLKvlffs5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pipe = Pipeline()\n",
    "pipe.add_component(\"text_embedder\", OpenAITextEmbedder(model=\"text-embedding-3-large\"))\n",
    "pipe.add_component(\n",
    "    \"retriever\", InMemoryEmbeddingRetriever(document_store=document_store)\n",
    ")\n",
    "pipe.add_component(\"chat_prompt_builder\", ChatPromptBuilder(template=template))\n",
    "pipe.add_component(\"llm\", llm)\n",
    "\n",
    "pipe.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "pipe.connect(\"retriever\", \"chat_prompt_builder.documents\")\n",
    "pipe.connect(\"chat_prompt_builder.prompt\", \"llm.messages\")"
   ],
   "metadata": {
    "id": "yl1CX1WD1oIq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3c697828-82f3-415f-a4b0-aa33b7d9a58a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:haystack.components.builders.chat_prompt_builder:ChatPromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7cc17be95190>\n",
       "ðŸš… Components\n",
       "  - text_embedder: OpenAITextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - chat_prompt_builder: ChatPromptBuilder\n",
       "  - llm: OpenAIChatGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> chat_prompt_builder.documents (List[Document])\n",
       "  - chat_prompt_builder.prompt -> llm.messages (List[ChatMessage])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the RAG System\n",
    "\n",
    "Let's test our complete RAG system with a question about the document content:"
   ],
   "metadata": {
    "id": "PsAoKhfrfl8m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "question = \"What are the benefits of watermarking protein generative models?\"\n",
    "response = pipe.run(\n",
    "    {\"text_embedder\": {\"text\": question}, \"chat_prompt_builder\": {\"question\": question}}\n",
    ")"
   ],
   "metadata": {
    "id": "ksvClKig1oEs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0abd8b10-7291-49e2-9c02-e5d921899f0e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:haystack.document_stores.in_memory.document_store:No Documents found with embeddings. Returning empty list. To generate embeddings, use a DocumentEmbedder.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected Output**: The system should provide a comprehensive answer about watermarking benefits in protein generative models, based on the content from the processed PDF. The answer will be grounded in the retrieved document chunks and should mention specific benefits like:\n",
    "- Intellectual property protection\n",
    "- Model attribution\n",
    "- Prevention of unauthorized use\n",
    "- Traceability of generated proteins"
   ],
   "metadata": {
    "id": "8rVvVV9RfoXq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(response[\"llm\"][\"replies\"][0].text)"
   ],
   "metadata": {
    "id": "P-eJdEPo78zi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7407e8de-0434-4e86-d71f-ca0d4da33b20"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The benefits of watermarking protein generative models include:\n",
      "\n",
      "1. **Provenance Tracking**: Watermarking allows for the identification of the source of generated proteins, helping trace their development and ensuring proper attribution to creators.\n",
      "\n",
      "2. **Ownership Protection**: It provides a means to protect intellectual property by distinguishing original work from unauthorized copies.\n",
      "\n",
      "3. **Quality Control**: Watermarks can serve as indicators of the model's reliability and performance, enabling users to assess the credibility of the generated proteins.\n",
      "\n",
      "4. **Detection of Misuse**: By embedding watermarks, developers can identify when their models are used without permission or inappropriately, allowing for accountability.\n",
      "\n",
      "5. **Enhanced Collaboration**: Watermarked models can foster collaborative efforts by making it easier to credit contributions and share advancements while maintaining ownership rights.\n",
      "\n",
      "6. **User Trust**: The presence of a watermark can increase user trust in the generated outputs, as it signals adherence to ethical standards and responsible usage of the model. \n",
      "\n",
      "These benefits collectively enhance the overall integrity and utility of protein generative models in research and application.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How the System Works\n",
    "\n",
    "This RAG system demonstrates several key concepts:\n",
    "\n",
    "1. **Document Processing**: Raw PDFs are converted into searchable, embedded chunks\n",
    "2. **Semantic Search**: Questions are matched to relevant content using vector similarity\n",
    "3. **Context-Aware Generation**: The LLM generates answers based on retrieved context\n",
    "4. **Modular Architecture**: Each component can be modified or replaced independently\n",
    "\n",
    "## Key Benefits of This Approach\n",
    "\n",
    "- **Accuracy**: Answers are grounded in your specific documents\n",
    "- **Transparency**: You can trace answers back to source chunks\n",
    "- **Scalability**: Can handle multiple documents and file types\n",
    "- **Flexibility**: Pipeline components can be easily modified or extended"
   ],
   "metadata": {
    "id": "MhRVC4gdf_DM"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "2HbeREitgAiv"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
